Status: 
Date Start: [[07-06-2022]]
Date Finish:
Author:
Tags: #book
***
# B - The car that knew too much

## Summary

## Notes
- If a self-driving car had to choose between a child and three adults, which group should it crash into to avoid killing the other?
- [[Moral Psychology]] - is the study of the mental processes that allow us to decide that one action is morally acceptable and that another is not.
- The experiment:
	- *One self driving car would have the choice between two accidents: going straight and killing ten pedestrians or swerving and hitting one?*
		- Which action is the most morally correct?
		- How do you want cars that drive on the road where you live to be programmed?
		- How would you want your own car to be programmed?
- In the first experiment, the majority of the population seemed to agree that the cars should be programmed to save the greatest number of people, even if that means sacrificing their own passenger. But, the people who answered that, also said they wouldn't be interested in buying such a car programmed to kill the passengers.
- This problem is a clear form of a social dilemma - in which individuals have the choice between cooperative act X or selfish act Y. It's better to live in a world where all do X rather than Y. However, all benefit if everyone acts selfish as well because no matter what others do, I prefer to live in a world where I do Y than where I do X.
	- According to the results of the experiment, people collectively prefer a society in which cars impartially save the greatest number of people, but they also want to make sure they buy a car that protects them - they want to do Y while everyone else does X.

## Quotes

## Random Tidbits
