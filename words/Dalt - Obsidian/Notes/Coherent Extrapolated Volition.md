CEV, was a term developed by EY while discussing [[Friendly AI development]]. It's meant as an argument to explain that we should program AI not with explicit instructions about what to or not to do through our desires and motivations, but have program it in a way where our best interests are in mind–and this is what guides its actions.

Volition - willpower; using one's will.

**What an idealized version of a person would want, separate from the context of any alignment problems.**

The thought experiment is posed something like this:

Say there are two boxes, A and B. Box B has a diamond in it, but only you know that fact.

Fred comes along and tries to guess which box has the diamond in it. Your job is to hand him the box he says.

Fred guesses box A.

By giving him Box A, you don't really help him out, because you know the right answer is Box B, but you're following explicit instructions that say you have to give Fred the box that he asks for.

That illustrates the CEV problem, because an AI must follow our volition, not just our decision or orders.

Another example, [posed here](https://web.archive.org/web/20131231151554/http://www.acceleratingfuture.com/michael/blog/2009/12/a-short-introduction-to-coherent-extrapolated-volition-cev/), is that a robot driver that was made to drive the children to school would be an idiot (and fired), if they drove the children to school when the school was on fire or dangerous. 

Humans have the ability to learn this through observation and discipline at a young age. If a child takes dessert before dinner and the parent yells at them to not, odds are they will understand what the parent is talking about and connect it to what they did–which is presumably wrong. But AI can't think like that. It doesn't know what it did wrong. Is the dessert evil? Should it have picked it up in a different way? What went wrong?

Some good questions about this, especially number 1, [taken from here](https://www.lesswrong.com/posts/wLmxiXfpLjiTBiT2j/two-questions-about-cev-that-worry-me):

1) One of the justifications for CEV was that extrapolating from an American in the 21st century and from Archimedes of Syracuse should give similar results. This seems to assume that change in human values over time is mostly "progress" rather than drift. Do we have any evidence for that, except saying that our modern values are "good" according to themselves, so whatever historical process led to them must have been "progress"?

2) How can anyone sincerely want to build an AI that fulfills anything except their own _current, personal_ volition? If Eliezer wants the the AI to look at humanity and infer its best wishes for the future, why can't he task it with looking at himself and inferring his best idea to fulfill humanity's wishes? Why must this particular thing be spelled out in a document like CEV and not left to the mysterious magic of "intelligence", and what other such things are there?

