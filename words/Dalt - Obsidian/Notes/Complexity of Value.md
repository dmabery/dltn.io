COV is the thesis that human values have high [[Kolmogorov Complexity]] - it's very difficult to sum up our preferences with a few simple rules. Something with a low Kolmogorov Complexity would be really simple to sum up and explain (usually referring to data sets.)

[[Fragility of value]] is that thesis that even losing a small part of the rules that make up our lives, the results are very, very different. If you build half a house, the result isn't ideal.


---
## On humans complexity of value

Let's pretend there is a magic machine that could grant you any lifestyle you asked it of. Professional golfer? Done. Star of *Pulp Fiction*? Say no more. Enlightened writer? Easy. Any lifestyle you wished to live could be yours upon demand. But like with any magical box or genie, there's a catch.

**The only thing it gave you was exactly what you asked it for.**

If you asked to be a star of *Pulp Fiction*, it could do that, but who knows what *Pulp Fiction* would become. It certaintly wouldn't be the genre-breaking film it was. Perhaps it would be a book about orange juice, but hey, you'd be the star!
You could be a professional golfer, but it would be 1100 AD and golf would be nowhere near the sport what it is today. It would take your prompt, but it would not give you the assumed preferences, the specifc things you value and care about, unless you *specifically* asked for it.

Let's look at a less trival example: that of the young couple wishing to get married, buy a house with a white picket fence, and have two kids with a dog. If this supposed couple presented the machine with that criteria for a lifestyle, they would get their house with a white picket fence and two kids, but they might be the only people on their side of the Mississippi. Since they didn't specify they wanted a car, or any other technology for that matter, they wouldn't get it.

But for the purpose of this thought experiment, before the young couple entered their lifestyle preferences into the machine, they had a teacher instruct them on some of the complications of the machine:

"Make sure to specify exaclty the kind of lifestyle you want to live. If you want to live in Paris in 1950, make sure to specify you want to live in 1950s Paris, just as it was in the real 1950s," says the teacher. "This machine will do *exactly* what you tell it to do with no assumed preferences or inheriteted human belief systems, so be specific and precise about the things you care about. Don't forget anything!"

## Complexity of value
The task of defining our every preference is difficult to say the least. If you didn't follow along with that thought experiment, just imagine all the different ideals and preferences you have about your current lifestyle. When I say preferences, I don't mean drinking lattes over americanos, though that is a valid preference. Instead, I mean how you define love, the legacy you wish to leave behind, and your defenition of living a "good life." You could try doing that for hours and you're guranteed to miss at least a couple of preferences. If you only rely on giving 5 or 10 things to the machine, life is going to look a lot different.

This idea, the fact that our preferences and values cannot be explained with just a few sentences has come to be known as our **Complexity of value**. [LessWrong](https://www.lesswrong.com/tag/complexity-of-value), a popular rationalist blog, explains that it's impossible for human values to be "summed up by a few simple rules, or compressed."

This is true because of the [fragility of our human values](https://www.lesswrong.com/posts/GNnHHmm8EzePmKzPk/value-is-fragile). If we all woke up tomorrow and *kindness* wasn't ever a thing, the world would be *wildly* different. 99.999% of everything else in the world could the exact same as it is today, but if we're missing kindness, or love, or humor, the world would cease to be the same. It might not be horrible, or evil, or bad, but it would most certanily be dull. Imagine a world without beauty. Everything, everywhere you look was just one shade of black or gray. Not black *and* gray but just different shades of either black or gray.

[Eliezer Yudkowsky](https://en.wikipedia.org/wiki/Eliezer_Yudkowsky), a computer scientist and one of the pioneers of some very fundamental AI theories and who wrote the post cited above explaining the fragility of our value puts it this way:

> Consider the [incredibly important human value of "boredom"](https://www.lesswrong.com/lw/xr/in_praise_of_boredom/) - our desire not to do "the same thing" over and over and over again.  You can imagine a mind that contained _almost_ the whole specification of human value, almost all the morals and metamorals, but left out _just this one thing_ -
> and so it spent until the end of time, and until the farthest reaches of its light cone, replaying a single highly optimized experience, over and over and over again
> Or imagine a mind that contained almost the whole specification of which sort of feelings humans most enjoy - but not the idea that those feelings had important _external referents._  So that the mind just went around _feeling_ like it had made an important discovery, _feeling_ it had found the perfect lover, _feeling_ it had helped a friend, but not actually _doing_ any of those things - having become its own experience machine.  And if the mind pursued those feelings _and their referents,_ it would be a good future and true; but because this _one dimension_ of value was left out, the future became something dull.  Boring and repetitive, because although this mind _felt_ that it was encountering experiences of incredible novelty, this feeling was in no wise true.

Humans are complex. Our values might be even more so. (Yes, this makes me wonder if something can be more of something than the thing that created it. I don't know the answer but it's interesting to think about).

## Kolmogorov Complexity
If we want to get really technical, which I always do, it can be said that human values have a high Kolmogorov complextiy. The Kolmogorov Complexity of something refers to the size of the shortest possible description of that thing. If the Kolmogorov Complexity is high, like we know our human values are, it's difficult to describe that thing in any shorter way than saying all of our actual values. Since our values are very complicated, we are unable to create a rule, theory, or formula to feed to the magical machine and say "Give us this lifestyle with these parameters," because we have to then go and define every single one of those parameters.

Inversly, if something is low in Kolmogorov Complexity, its description can be written in a shorter way than the actual output.

Let's pretend you and I went out for chinese food after a long day at the office. I don't like fortune cookies, so I gave you mine and said you could pick the best one. Here they are:

`dmdmdmdmdmdmdmdmdmdmdmdmdmdmdmdm`
`83jsns02u84nlw9jsn48anw2kfmv8940`

After reading both of them, I ask you what the first one said. You could say 
`"It says dm 32 times"`, which has only 19 characters and is a simple description. Now I ask you what the second one said. The only real way to describe it is to say aloud the random string of characters. Naturally, the first set is less complex than the second set because the description of it is shorter than the output. Remember, Kolmogorov Complexity refers to the size of the shortest possible description of the output that *gives the same* input. If for some reason I wanted to copy the fortunes, you could have said "Write dm 32 times." That's a very short input for the subsequent output. Conversly, the input for the second string, "Write `83jsns02u84nlw9jsn48anw2kfmv8940` " is longer than the original output, so the Kolmogorov Complexity is high. It's interesting though, that the complexity of any string cannot be much higher than the length of the string itself. Even with our high complexity string, the input is only 6 more characters long.

## Why this matters
Calculating the Kolmogorov Complexity of 32 character strings is not exciting, but when using Kolmogorov Complexity as a model as to how the world works, it gets a bit more exciting. You start to understand the intricacies of everyday life and see how the world is actually really complicated, no matter what someone else says.

It's hard to accurately represent something using just a map or description of that thing. As we start to realize that more and more, we understand arguments run much deeper than what's 

