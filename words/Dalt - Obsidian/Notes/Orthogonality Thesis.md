This states that an artificial intelligence can have any combination of intelligence level and goal, or in more AI-terms, utility function and General Intelligence. So just because an AI is very, very intelligent, doesn't mean it will want to do, or do, "intelligent" things in its own right, because they are separate functions of the AI.

Example: [[Paperclip maximizer]], and [[Instrumental Convergence]]

This contrasts with the idea that all AIs will converge to some common goal if they all have the same amount of general intelligence.

**Naturally, there are some defenses to this thesis:**

Most people assume Ai will converge to the same goals because most humans, at a high level, value similar things. Combine that with the fact that most worldviews assume there is a rationally correct morality, which assumes any AI will adopt this morality and begin to act accordingly.